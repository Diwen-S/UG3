[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Practice",
    "section": "",
    "text": "1 Introduction\nThis quarto file is used to practice my formatting skills as much as possible.\nThus, the content may be displayed in an esoteric pattern like this.\nChapter 2\nFigure 3.1"
  },
  {
    "objectID": "Stata_and_R.html#robust-standard-errors",
    "href": "Stata_and_R.html#robust-standard-errors",
    "title": "2  Stata and R",
    "section": "2.1 Robust Standard Errors",
    "text": "2.1 Robust Standard Errors\nIn probit estimation, Stata and R will produce difference robust standard errors.\n\n\nCode\nuse \"E:/Curriculum001/UG3/EC338/Assignment2/Smoking.dta\"\nprobit smoker i.smkban age i.hsdrop i.hsgrad i.colsome i.colgrad i.black i.hispanic if female == 0,robust\n\n\nIteration 0:   log pseudolikelihood = -2489.3259  \nIteration 1:   log pseudolikelihood =  -2346.727  \nIteration 2:   log pseudolikelihood = -2345.1465  \nIteration 3:   log pseudolikelihood = -2345.1464  \n\nProbit regression                                       Number of obs =  4,363\n                                                        Wald chi2(8)  = 263.16\n                                                        Prob &gt; chi2   = 0.0000\nLog pseudolikelihood = -2345.1464                       Pseudo R2     = 0.0579\n\n------------------------------------------------------------------------------\n             |               Robust\n      smoker | Coefficient  std. err.      z    P&gt;|z|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n    1.smkban |   -.197581   .0427786    -4.62   0.000    -.2814255   -.1137366\n         age |  -.0032241   .0017416    -1.85   0.064    -.0066377    .0001894\n    1.hsdrop |   1.095518   .0992209    11.04   0.000     .9010486    1.289987\n    1.hsgrad |   .9249239   .0836137    11.06   0.000      .761044    1.088804\n   1.colsome |   .6861135   .0851945     8.05   0.000     .5191354    .8530916\n   1.colgrad |   .3216797   .0894715     3.60   0.000     .1463188    .4970405\n     1.black |  -.0003162   .0840926    -0.00   0.997    -.1651348    .1645023\n  1.hispanic |  -.2377863   .0691798    -3.44   0.001    -.3733763   -.1021963\n       _cons |  -1.067899   .1068883    -9.99   0.000    -1.277396    -.858402\n------------------------------------------------------------------------------\n\n\n\n\nCode\nreg91 &lt;- glm(smoker ~ as.factor(smkban) + age + hsdrop + hsgrad + colsome+ colgrad + \n               black + hispanic,data = data01,family = binomial(link=\"probit\"), \n             subset = female==0)\ncoeftest(reg91, vcov = vcovHC(reg91, type=\"HC1\"))\n\n\n\nz test of coefficients:\n\n                      Estimate  Std. Error  z value  Pr(&gt;|z|)    \n(Intercept)        -1.06789853  0.10634581 -10.0418 &lt; 2.2e-16 ***\nas.factor(smkban)1 -0.19758088  0.04287431  -4.6084 4.058e-06 ***\nage                -0.00322415  0.00172811  -1.8657  0.062083 .  \nhsdrop              1.09551672  0.09983792  10.9730 &lt; 2.2e-16 ***\nhsgrad              0.92492334  0.08379179  11.0384 &lt; 2.2e-16 ***\ncolsome             0.68611331  0.08541225   8.0330 9.515e-16 ***\ncolgrad             0.32167916  0.08968817   3.5866  0.000335 ***\nblack              -0.00031533  0.08447431  -0.0037  0.997022    \nhispanic           -0.23778551  0.07042242  -3.3766  0.000734 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1"
  },
  {
    "objectID": "Stata_and_R.html#plot",
    "href": "Stata_and_R.html#plot",
    "title": "2  Stata and R",
    "section": "2.2 Plot",
    "text": "2.2 Plot\nA arbitary graph draw from EC338 Assignment 1\n\ns3t1f[[1]]\ns3t1f[[2]]\n\n\n\n\n\n\nFigure 2.1: Arbitary\n\n\n\n\n\n\n\nFigure 2.2: Arbitary"
  },
  {
    "objectID": "summary.html#proofreader",
    "href": "summary.html#proofreader",
    "title": "3  Mathtype",
    "section": "3.1 Proofreader",
    "text": "3.1 Proofreader\n\n\n\nFigure 3.1: Linear\n\n\nAusten-Smith, D. & Banks, J.S. 2005, Positive political theory II: strategy and structure, University of Michigan Press, Ann Arbor, [Mich.]."
  },
  {
    "objectID": "Seminar_4.html#application-1-clustering-standard-errors",
    "href": "Seminar_4.html#application-1-clustering-standard-errors",
    "title": "4  Seminar 4:Clustering Standard Errors and Models with Binary Outcomes",
    "section": "4.1 Application 1: Clustering Standard Errors",
    "text": "4.1 Application 1: Clustering Standard Errors\n\n4.1.1 General setup\nOur first application will use two simulated datasets on student achievement with the following variables:\n\ny is the outcome variable: a measure of student achievement\ni is an individual-level identifier\nclassroom is a classroom-level identifier\naircon is a binary variable equal to one if the classroom has air conditioning\n\nThe goal in this application is to understand that, if we think that our data is characterised by group-level shocks, we will need to adjust the standard errors. In particular, basic standard errors are calculated on the presumption of iid error terms. However, in many applications we may think of our data being organised in a number of groups (called clusters), and the unobservables may be correlated within these groups.\nWhy is that?\nThink about the example from this application. Suppose you want to estimate the effect that air conditioning has on student achievement: \\(y = \\alpha + \\beta aircon_i + η_{(c)i} + \\epsilon_i\\). We have two unobservables here: a classroom-level shock η(c)i and an individual shock \\(\\epsilon\\_i\\) . You can think of the regression unobservables as being denoted by \\(u_i = \\eta_{(c)i} + \\epsilon_i\\). The key issue in this example (and with clustering in general) is that, since two students in the same class will be exposed to the same classroom-level shock, our error term ui will no longer satisfy the iid assumption. If we do not adjust our standard errors accordingly, inference on the parameters in our model can go badly wrong.\n\\[\n\\Omega=\n\\begin{bmatrix}\n\\sigma^2 & 0 & 0 & \\cdots & 0 \\\\\n0 & \\sigma^2 & 0 & \\cdots & 0 \\\\\n0 & 0 & \\sigma^2 & \\cdots & 0 \\\\\n\\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\\n0 & 0 & 0 & \\cdots & \\sigma^2\n\\end{bmatrix}\n\\] ## Clustering of Standard Error In the first part, we will look at the dataset aircon1 where, by construction, the iid assumption of our error terms holds.\n\n*** import dataset 1\nimport excel using \"aircon1\", first\n\n*** estimating the effect of air conditioning on achievement\n\n* start with the simplest model, where assume iid errors\nreg y aircon\n\n(4 vars, 5,000 obs)\n\n      Source |       SS           df       MS      Number of obs   =     5,000\n-------------+----------------------------------   F(1, 4998)      =   1298.54\n       Model |  1276.01233         1  1276.01233   Prob &gt; F        =    0.0000\n    Residual |  4911.29211     4,998  .982651483   R-squared       =    0.2062\n-------------+----------------------------------   Adj R-squared   =    0.2061\n       Total |  6187.30445     4,999  1.23770843   Root MSE        =    .99129\n\n------------------------------------------------------------------------------\n           y | Coefficient  Std. err.      t    P&gt;|t|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n      aircon |   1.010351   .0280379    36.04   0.000     .9553849    1.065318\n       _cons |   4.989557   .0198258   251.67   0.000      4.95069    5.028424\n------------------------------------------------------------------------------\n\n\n\nlibrary(readxl)\ndata4 &lt;- read_excel(\"aircon1.xls\")\nsummary(lm(y ~ aircon, data = data4))"
  },
  {
    "objectID": "Appendix.html",
    "href": "Appendix.html",
    "title": "5  Appendix",
    "section": "",
    "text": "[definition] Variance equation: \\(A\\) is a non-stochastic matrix, \\(y\\) is a stochastic matrix \\[\n\\begin{aligned}\nVar(Ay) &=A*Var(y)*A'   \\\\\nVar(\\hat{\\beta})&=Var((X'X)^{-1}X'y)    \\\\\n&=(X'X)^{-1}X'\\sigma^2IX(X'X)^{-1}\\\\\n&=\\sigma^2(X'X)^{-1}X(X'X)^{-1}\\\\\n&=\\sigma^{2}(X'X)^{-1}\n\\end{aligned}\n\\]\n\nIn iid, where all observations are independent to each other, the variance-covariance looks like. \\[\n\\Omega=\n\\begin{bmatrix}\n\\sigma^2 & 0 & 0 & \\cdots & 0 \\\\\n0 & \\sigma^2 & 0 & \\cdots & 0 \\\\\n0 & 0 & \\sigma^2 & \\cdots & 0 \\\\\n\\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\\n0 & 0 & 0 & \\cdots & \\sigma^2\n\\end{bmatrix}\n= \\sigma^2I\n\\]\nSynthetic control with R\n\n\nCode\ndf341&lt;-read_dta('sectionC_task4_collapseddata.dta')\ndf34 &lt;- df341 %&gt;%\n  group_by(province, year) %&gt;%\n  summarise(across(starts_with(\"edu\"), ~mean(.x, na.rm = TRUE)), \n            formar = mean(formar, na.rm = TRUE), \n            cohab = mean(cohab, na.rm = TRUE))\ndf34$name &lt;- \"\"\nproname &lt;- c(\"Newfoundland & Labrador\", \"Prince Edward Island\", \"Nova Scotia\", \"New Brunswick\", \"Quebec\",\"Ontario\",\"Manitoba\",\" Saskatchewan\",\"Alberta\" ,\"British Columbia\")\nfor (i in 1:10){\ndf34$name &lt;- ifelse(df34$province == i, proname[i],df34$name)\n} \n\ndf34 &lt;- as.data.frame(df34)\ndf34$province &lt;- as.numeric(df34$province)\n# Prepare the data\nt3synp1 &lt;- dataprep(\n   foo = df34,\n   predictors    = c(\"edu12\", \"edu13\", \"edu21\", \"edu22\", \"edu23\", \"edu31\", \"edu32\", \"edu33\"),\n   predictors.op = \"mean\",\n   dependent     = \"formar\",\n   unit.variable = \"province\",\n   time.variable = \"year\",\n   unit.names.variable = \"name\",\n   treatment.identifier  = 10,\n   controls.identifier   = c(1:9),\n   time.predictors.prior = c(2000:2012),\n   time.optimize.ssr     = 2000:2013,\n   time.plot             = 2000:2019\n)\n\nt3syn1 &lt;- synth(t3synp1)\n\n\n\nX1, X0, Z1, Z0 all come directly from dataprep object.\n\n\n**************** \n searching for synthetic control unit  \n \n\n**************** \n**************** \n**************** \n\nMSPE (LOSS V): 9.723109e-05 \n\nsolution.v:\n 6.1906e-06 0.01370176 3.0543e-06 0.04109318 0.3385545 0.298815 0.3066765 0.001149889 \n\nsolution.w:\n 4.5251e-06 0.004146942 0.00410196 0.005293371 0.0009749935 0.2324437 0.04271549 0.003839381 0.7064796 \n\n\nCode\nt3syng1 &lt;- path.plot(dataprep.res = t3synp1,synth.res = t3syn1,Ylim = c(0.66,0.87), Legend = c(\"10.BC\",\"synthetic 10. BC\"))\n\n\n\n\n\nCode\nt3synp2 &lt;- dataprep(\n   foo = df34,\n   predictors    = c(\"cohab\",\"edu12\", \"edu13\", \"edu21\", \"edu22\", \"edu23\", \"edu31\", \"edu32\", \"edu33\"),\n   predictors.op = \"mean\",\n   special.predictors = list(\n      list(\"formar\", 2009, \"mean\")),\n   dependent     = \"formar\",\n   unit.variable = \"province\",\n   time.variable = \"year\",\n   unit.names.variable = \"name\",\n   treatment.identifier  = 10,\n   controls.identifier   = c(1:9),\n   time.predictors.prior = c(2000:2012),\n   time.optimize.ssr     = 2000:2013,\n   time.plot             = 2000:2019\n)\nt3syn2 &lt;- synth(t3synp2)\n\n\n\nX1, X0, Z1, Z0 all come directly from dataprep object.\n\n\n**************** \n searching for synthetic control unit  \n \n\n**************** \n**************** \n**************** \n\nMSPE (LOSS V): 9.616666e-05 \n\nsolution.v:\n 0.200451 0.03583939 0.04740207 0.08822844 0.01968502 0.1645045 0.1040342 0.1253762 0.1179669 0.09651227 \n\nsolution.w:\n 6.826e-07 2.5922e-06 0.07121743 1.4201e-06 3.981e-06 0.4011764 3.69393e-05 3.9048e-06 0.5275567 \n\n\nCode\nt3syng2 &lt;- path.plot(dataprep.res = t3synp2,synth.res = t3syn2, Ylim = c(0.66,0.87), Legend = c(\"10.BC\",\"synthetic 10. BC\")) \n\n\n\n\n\nCode\nt3synp3 &lt;- dataprep(\n   foo = df34,\n   predictors    = c(\"cohab\", \"edu12\", \"edu13\", \"edu21\", \"edu22\", \"edu23\", \"edu31\", \"edu32\", \"edu33\"),\n   predictors.op = \"mean\",\n   special.predictors = list(\n      list(\"formar\", 2007, \"mean\"),\n      list(\"formar\", 2009, \"mean\"),\n      list(\"formar\", 2011, \"mean\")),\n   dependent     = \"formar\",\n   unit.variable = \"province\",\n   time.variable = \"year\",\n   unit.names.variable = \"name\",\n   treatment.identifier  = 10,\n   controls.identifier   = c(1:9),\n   time.predictors.prior = c(2000:2012),\n   time.optimize.ssr     = 2000:2011,\n   time.plot             = 2000:2019\n)\nt3syn3 &lt;- synth(t3synp3)\n\n\n\nX1, X0, Z1, Z0 all come directly from dataprep object.\n\n\n**************** \n searching for synthetic control unit  \n \n\n**************** \n**************** \n**************** \n\nMSPE (LOSS V): 6.475819e-05 \n\nsolution.v:\n 0.1823591 0.1522035 0.01560817 0.129904 0.02137578 0.06901198 0.06846723 0.06128481 0.09794489 0.06594726 0.06304812 0.07284508 \n\nsolution.w:\n 7.232e-07 1.1139e-06 0.1728935 5.951e-07 1.5768e-06 0.5146288 3.97503e-05 1.4187e-06 0.3124325 \n\n\nCode\nt3syng3 &lt;- path.plot(dataprep.res = t3synp3,synth.res = t3syn3, Ylim = c(0.66,0.87), Legend = c(\"10.BC\",\"synthetic 10. BC\"))"
  }
]